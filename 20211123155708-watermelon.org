:PROPERTIES:
:ID:       f4fd38b3-2378-45d7-b931-e94eef78396f
:END:
#+title: watermelon

* Intro
** Intro
**Machine learning** means "improving system performance by learning from experience via computational methods"

**learning algorithms** build **models** from **data**

**models** make **predictions** on **new observations**
** Terminology
A **data set** is a colletion of **records** (or **instances** / **samples**)

each **record** contains the description of an event or object (**attributes** or **features**).
A **record** is also called a **feature vector**, as it is defined as a collection of **features**

to **train** (or **learn**) a model, we use **training data**
(where each sample is a **training example**, and the set of all samples is called the **training set*)

A **learned model** is also called a **hypothesis**
Models are also sometimes called **learners** (machine learning algorithms instantiated with data and parameters)
Goal: find or approximate **ground-truth** 

Outcome of a sample is called its **label**
Sample + label = **example**

When prediction output is discrete, the problem is a **classification** problem (binary if only two possible classes, multiclass if there are more)
When prediction output is continuous, the problem is a **regression** problem.

A **prediction problem** (supervised learning) is to establish a mapping $f : x \Rightarrow Y$
from the input space $x$ to the output space $y$, learning from a (labeled) training set 

**Testing** means making predicitons from a trained (learned) model

In contrast to classification, **Clustering** (unsupervised) clusters unlabeled data sharing traits (e.g PCA), and can provide insights that form the basis
for further analysis

**Generalization** is the ability for a model to work on samples outside of the training set

Generally assume our training data is a good representation of the (assumed to be i.i.d) distribution of all samples

More training samples => better-generalized model
** Hypothesis space
**Induction** moves from specialization to generalization
Learning from examples is **inductive** (known as **inductive learning**)

We want to learn **concepts** from training data (this is hard, trained models are often black boxes)

**Machine learning** can also be defined as the search in the **hypothesis space** for a **hypothesis** that is consistent with thetraining set.

Hypothesis space is huge, training exmaples are finite. There can be a set of multiple hypotheses (**version space**) that are all consistent with the training data.

Remember that here a **hypothesis** is just a trained model.
** Inductive bias
Defined as the bias of a learning algorithm toward a particular class of hypotheses
(given a **version space** of hypotheses, the algorithm must select one model out of any number that are all equally consistent with the training data)

All algorithms must have a bias, a bias-free algorithm would randomly choose models from the version space, which introduces non-determinism (bad)

**Inductive bias** can be thought of as the 'heuristic' or value philosophy of a learning algorithm

Occam's razor is commonly employed, (smoother is simpler, simpler is better) in deciding inductive bias, although there are instances where this leads the
model to be inconsistent with new testing data (**out-of-sample error**).

**no-free-lunch theorem**: "any two optimization algorithms are equivalent when their performance is averaged across all possible problems"

Corollary to NFL: debating "which ML algorithm is better" is meaningless without considering the algorithms with respect to a certian problem
(all algs are equally as good considering all contexts).

Hence, inductive bias must be considerd on a case-by-case basis.
** History
"Reasoning age" (50s-early 70s): It was believed that Logical reasoning => intelligence

Later in the 70s, it was realized that intelligence was more complicated than just reasoning (:()

"Knowledge age" (mod-late 70s): developement of expert systems 

idk lol a buncha stuff happened
Michalski in 1983 divided/classified machine learning methods

late 80s (reasoning age) saw mainstream of learning from examples as **symbolism learning** (decision trees, logic-based learning) 

buncha stuff we will get to later I think
** Application status
ML is now very important in 'applied' areas (NLP, CV), and also provides support for interdisciplinary research (e.g bioinformatics)

science is shifting from theory + experiment to theory + experiment + computation ("data science")

ML allows us to work with very large data sets.

**Data minig** is about knowledge discovery from massive datasets, influences statistics via ML

It's important, ok?

Also important to further understand "how humans learn"
** Excercises
*** What is the version space if Table 1.1 contains only the examples 1 and 4?
(color=*, root=*, sound=*)
(color=*, root=curly, sound=*)
(color=*, root=*, sound=muffled)
(color=*, root=*, sound=mufled)
(color=green, root=*, sound=*)
(color=green, root=*, sound=mufled)
(color=green, root=curly, sound=mufled)
you get the gist
*** If we use at most $k$ conjunction clauses to express the hypothesis space of the watermellon classification problem in Table 1.1, what is the total number of possible hypotheses?
ma
1 + ($k$ + 1)!
*** Inductive bias when data is noisy 
Take 'simpler' model (fewer layers, fewer parameters). Prefer consistent models over ones with breakout successes
***  Change misclassification error rate, prove NFL still works
Because we are iterating over *all* samples not in the training set, changing the performance measurement just 'swaps' some values in the summation
and because this is a (supposedly) finite summation, changing the order does not affect the value of the summation.
(we only care, to prove NFL, that the algs are equivalent- not their exact value)
*** Describe ML in differnet subcomponents of internet search
autocomplete- how do people searching these first phrases tend to end their questions / entries?
search ordering- which sites do people with a similar question / phrase tend to click on?
ads- same as search ordering


* Model selection and Validation
** Empirical error and overfitting 
The **error rate** is the proprtion of incorrectly classified samples ($E = \frac a m, where $a$ out of $m$ samples are misclassified).
Accuracy is the complement of the error rate

The error calculated on the training set is called **empirical error**, while the error found on new samples is called **generalization error**

Often when **empirical error** is zero (model perfectly fits training data) **generalization error** rears its ugly head
and we are faced with an **overfitted** model. (overfitting is learning the training data ``too well'')

The opposite of **overfitting**, of course, is **underfitting**, which occurs when the learner fails to learn general properties of the training examples

Overfitting commonly stems from learning non-general peculiarities of training examples, rather than just the common traits

Usually, machine learning problems are NP-Hard (e.g playing Super Mario Bros.), but many learning algorithms run in polynomial time.

Thus, if we can show that overfitting can be avoided, we have a constructive proof for P=NP

Underfitting is easier to overcome. Just train more lol

** Evaluation methods

Set some data aside during learning as a **testing set**, and use the error against that set (**testing error**) as an approximation for
the generalization error.

This assumes samples are independently and identically samples from the ground-truth distribution.

Works best if testing set and training set are as independent as possible.

Strategies for splitting the dataset D into a training set S and a testing set T include
*** Hold-Out
Split D into two disjoint subsets: S and T ($D = S \cup T$ and $S \cap T = \emptyset$)

The split ratio is always a tradeoff- usually between 2/3 and 4/5 of the samples are used for training tho
We can (and usually) do multiple differnet splits of D (even at the same ratio) and average the testing error across them
*** Cross-Validation
Split D into $k$ disjoint subsets with similar sizes.

In each trial of cross-validation, use the union of $k-1$ sets as the training, and the last set as the testing set.

Repeat the process $k$ times, so each subset is used as the testing set exactly once.

Average over $k$ trials to get our final **testing error**.

This is also known as $k$-fold cross-validation

$k$ is commonly chosen as 10, but other common values include 5 and 20

Often, the data is split into the $k$ subsets $p$ times and averaged again to get $p$-time $k$-fold cross-validation

Special case of cross-validation: Leave-One-Out (LOO), where for a dataset D of $m$ samples, $k$ = $m$
LOO is good, but has a very high computational cost (training $m$ models)- not necessarily more accurate (no free lunch!)
*** Bootstrapping 
Bootstrapping trains on $m$ samples randomly chosen (with replacement) from D of size $m$, and uses the samples not picked (~36.8%) as the testing set.
This is useful when the dataset is small, or we can't really split it u.

Bootstrapping is biased tho (because it changes the original data distribution our model is trained on),
so we usually don't use it when we have abundant data.
*** Parameter tuning and final model
for models with (usually real-valued) (hyper?)parameters, we usually split out an extra disjoint subset from D, called the **validation set**, which we
use to optimize parameters (with a step size cuz real values).
This gives us a **training set** for the model, a **validation set** for the parameters, and a **testing set** to estimate generalization error.
